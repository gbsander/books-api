# Books API - Tech Challenge

API RESTful para consultar dados de livros extra√≠dos do site [books.toscrape.com](https://books.toscrape.com/).

## üìã Descri√ß√£o do Projeto

Este projeto implementa um **pipeline completo de dados**:
1. **Web Scraping**: Extra√ß√£o automatizada de dados de livros
2. **Armazenamento**: Dados salvos em formato CSV
3. **API REST**: Endpoints para consulta e busca de livros
4. **Documenta√ß√£o**: Swagger/OpenAPI autom√°tico

## üèóÔ∏è Arquitetura

```
tc1_/
‚îú‚îÄ‚îÄ scripts/           # Web scraping
‚îÇ   ‚îî‚îÄ‚îÄ scraper.py    # Script de extra√ß√£o de dados
‚îú‚îÄ‚îÄ api/              # API RESTful
‚îÇ   ‚îú‚îÄ‚îÄ main.py       # Aplica√ß√£o FastAPI
‚îÇ   ‚îú‚îÄ‚îÄ models.py     # Modelos Pydantic
‚îÇ   ‚îî‚îÄ‚îÄ services.py   # L√≥gica de neg√≥cio
‚îú‚îÄ‚îÄ data/             # Armazenamento
‚îÇ   ‚îî‚îÄ‚îÄ books.csv     # Dados extra√≠dos
‚îî‚îÄ‚îÄ requirements.txt  # Depend√™ncias
```

### Pipeline de Dados
```
[Site] ‚Üí [Scraper] ‚Üí [CSV] ‚Üí [API] ‚Üí [Cliente]
```

## üöÄ Instala√ß√£o e Configura√ß√£o

### 1. Clonar o reposit√≥rio
```bash
git clone <seu-repositorio>
cd tc1_
```

### 2. Criar ambiente virtual
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate  # Windows
```

### 3. Instalar depend√™ncias
```bash
pip install -r requirements.txt
```

### 4. Executar o scraping
```bash
python scripts/scraper.py
```
Isso vai gerar o arquivo `data/books.csv` com todos os livros.

### 5. Rodar a API
```bash
uvicorn api.main:app --reload
```

A API estar√° dispon√≠vel em: `http://localhost:8000`

## üìö Documenta√ß√£o da API

### Documenta√ß√£o Interativa (Swagger)
Acesse: `http://localhost:8000/docs`

### Endpoints Dispon√≠veis

#### 1. Health Check
```http
GET /api/v1/health
```

**Resposta:**
```json
{
  "status": "ok",
  "total_books": 1000
}
```

#### 2. Listar Todos os Livros
```http
GET /api/v1/books
```

**Resposta:**
```json
[
  {
    "id": 1,
    "title": "A Light in the Attic",
    "price": 51.77,
    "rating": 3,
    "availability": "In stock",
    "image_url": "https://books.toscrape.com/media/cache/..."
  },
  ...
]
```

#### 3. Buscar Livro por ID
```http
GET /api/v1/books/{id}
```

**Exemplo:**
```bash
curl http://localhost:8000/api/v1/books/1
```

**Resposta:**
```json
{
  "id": 1,
  "title": "A Light in the Attic",
  "price": 51.77,
  "rating": 3,
  "availability": "In stock",
  "image_url": "https://books.toscrape.com/media/cache/..."
}
```

#### 4. Buscar Livros por T√≠tulo
```http
GET /api/v1/books/search?title={termo}
```

**Exemplo:**
```bash
curl "http://localhost:8000/api/v1/books/search?title=light"
```

**Resposta:**
```json
[
  {
    "id": 1,
    "title": "A Light in the Attic",
    ...
  }
]
```

#### 5. Listar Categorias
```http
GET /api/v1/categories
```

**Exemplo:**
```bash
curl http://localhost:8000/api/v1/categories
```

**Resposta:**
```json
[
  {
    "name": "1 Star",
    "count": 226
  },
  {
    "name": "2 Stars",
    "count": 196
  },
  ...
]
```

## üß™ Exemplos de Uso

### Usando cURL
```bash
# Health check
curl http://localhost:8000/api/v1/health

# Listar todos
curl http://localhost:8000/api/v1/books

# Buscar por ID
curl http://localhost:8000/api/v1/books/5

# Buscar por t√≠tulo
curl "http://localhost:8000/api/v1/books/search?title=python"

# Listar categorias
curl http://localhost:8000/api/v1/categories
```

### Usando Python
```python
import requests

# Buscar todos os livros
response = requests.get("http://localhost:8000/api/v1/books")
books = response.json()
print(f"Total de livros: {len(books)}")

# Buscar livro espec√≠fico
response = requests.get("http://localhost:8000/api/v1/books/1")
book = response.json()
print(f"Livro: {book['title']}")

# Buscar por t√≠tulo
response = requests.get("http://localhost:8000/api/v1/books/search",
                       params={"title": "light"})
results = response.json()
print(f"Encontrados: {len(results)} livros")
```

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.8+**
- **FastAPI**: Framework web moderno e r√°pido
- **Uvicorn**: Servidor ASGI
- **BeautifulSoup4**: Web scraping
- **Pandas**: Manipula√ß√£o de dados
- **Pydantic**: Valida√ß√£o de dados

## üìä Estrutura dos Dados

Cada livro possui os seguintes campos:

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| id | int | Identificador √∫nico |
| title | string | T√≠tulo do livro |
| price | float | Pre√ßo em libras (¬£) |
| rating | int | Avalia√ß√£o de 1 a 5 estrelas |
| availability | string | Status de disponibilidade |
| image_url | string | URL da imagem da capa |

## üîÑ Escalabilidade e Melhorias Futuras

### Melhorias Implement√°veis:
1. **Banco de Dados**: Migrar de CSV para PostgreSQL/MongoDB
2. **Cache**: Implementar Redis para respostas r√°pidas
3. **Pagina√ß√£o**: Adicionar limit/offset nos endpoints
4. **Filtros**: Adicionar filtros por pre√ßo, rating, etc
5. **Autentica√ß√£o**: JWT para endpoints protegidos
6. **Rate Limiting**: Limitar requisi√ß√µes por IP
7. **Testes**: Pytest para cobertura de testes
8. **CI/CD**: GitHub Actions para deploy autom√°tico

### Integra√ß√£o com Machine Learning:
```python
# Exemplo de uso para ML
import pandas as pd

# Carregar dados via API
response = requests.get("http://localhost:8000/api/v1/books")
df = pd.DataFrame(response.json())

# Preparar features para modelo
X = df[['price', 'rating']]
y = df['availability']

# Treinar modelo de classifica√ß√£o
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier()
model.fit(X, y)
```

## üöÄ Deploy

### Deploy no Render (Gratuito)

1. Criar conta no [Render](https://render.com)
2. Criar novo Web Service
3. Conectar ao reposit√≥rio GitHub
4. Configurar:
   - **Build Command**: `pip install -r requirements.txt && python scripts/scraper.py`
   - **Start Command**: `uvicorn api.main:app --host 0.0.0.0 --port $PORT`

### Deploy no Fly.io

```bash
# Instalar Fly CLI
curl -L https://fly.io/install.sh | sh

# Fazer login
fly auth login

# Deploy
fly launch
fly deploy
```

## üìù Licen√ßa

Este projeto foi desenvolvido para fins educacionais como parte do Tech Challenge.

## üë®‚Äçüíª Autor

[Seu Nome]
- GitHub: [@seu-usuario](https://github.com/seu-usuario)
- LinkedIn: [Seu LinkedIn](https://linkedin.com/in/seu-perfil)

---

**Observa√ß√£o**: Este projeto realiza web scraping de forma √©tica e respeitosa, com delays entre requisi√ß√µes para n√£o sobrecarregar o servidor alvo.
